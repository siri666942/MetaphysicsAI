# RAG 质量增强方法

针对当前命理知识库（chunks + baihua + 关键词检索）的增强思路，按「素材 → 切分 → 检索 → 使用」分层。

---

## 一、素材与数据质量

| 方法 | 说明 | 优先级 |
|------|------|--------|
| **用白话文做 chunk 源** | 用 `baihua/*.txt` 替代 raw 做切分，检索到的就是白话，模型更好用。 | 高 |
| **补全 tags** | 为每个 chunk 打上命理标签（如 十神、月令、格局、刑冲合害），检索时命中更准。 | 高 |
| **同义/别名扩展** | 在 chunk 的 tags 或 content 里加入同义词（如「日主」⇔「日干」、「七杀」⇔「偏官」），或检索前对 query 做同义扩展。 | 中 |
| **原文+白话并存** | chunk 内保留「原文 + 白话」两段，检索到后按需只给模型白话，或都给。 | 中 |

---

## 二、切分策略

| 方法 | 说明 | 优先级 |
|------|------|--------|
| **按章节/标题切** | 按《论XXX》等标题切块，保证语义完整，便于打 tag。 | 高 |
| **控制块长** | 单块 300–800 字为宜，太长易掺无关信息，太短缺上下文。 | 高 |
| **重叠切分** | 相邻块保留少量重叠（如 50 字），减少边界截断。 | 低 |
| **关键句摘要** | 每块写一句「本段讲什么」，放入 chunk 或单独字段，便于检索。 | 中 |

---

## 三、检索增强

| 方法 | 说明 | 优先级 |
|------|------|--------|
| **关键词 + 命理术语扩展** | 对用户 query 自动补全术语（如问「冲」补「六冲、子午冲、刑冲」），再匹配。 | 高 |
| **向量检索（embedding）** | 用 OpenAI/本地模型算 chunk 与 query 的向量，做语义相似度检索，与关键词取并集或重排。 | 高 |
| **混合检索** | 先关键词筛一批，再用向量相似度重排（rerank），取 top_k。 | 高 |
| **分书检索** | 若问「子平真诠里用神怎么取」，优先从对应书的 chunk 里取，再补其他书。 | 中 |
| **去重与截断** | 多 chunk 内容重复时去重；总长度超模型上下文时按相关性截断。 | 中 |

---

## 四、使用方式（注入 prompt）

| 方法 | 说明 | 优先级 |
|------|------|--------|
| **明确标注来源** | 每条参考写清「来源：渊海子平 · 论伤官」，减少模型幻觉。 | 高 |
| **控制总长度** | 参考总字数设上限（如 2000 字），避免占满上下文。 | 高 |
| **按相关性排序** | 最相关的放前面，便于模型优先利用。 | 中 |
| **少而精** | top_k 不必大，3–5 条高质量片段往往优于 10 条泛泛。 | 中 |

---

## 五、实施顺序建议

1. **短期（不改架构）**  
   - 用 baihua 重新生成 chunks（或仅对三命/滴天等用 baihua 再切一遍）。  
   - 在 `rag.py` 里加强 query 扩展（更多命理术语、同义词）。  
   - 为现有优质 chunk（如 ziwu_chong）补 tags，并以此为模板给新 chunk 打 tag。

2. **中期（增强检索）**  
   - 引入 embedding + 向量库（Chroma/FAISS/pgvector），做语义检索。  
   - 实现「关键词初筛 + 向量 rerank」混合。

3. **长期（可选）**  
   - 用交叉编码器或小模型做 rerank。  
   - 对高频问题维护「问答对 + 指定 chunk」，做精准召回。

---

## 六、与本仓库脚本的对应关系

- **素材**：`raw/` 原文 → `scripts/raw_to_baihua.py` → `baihua/` 白话。  
- **切分**：`scripts/raw_to_chunks.py` 从 raw 切分；`scripts/baihua_to_chunks.py` 从 `baihua/` 切分并写入带 `_baihua_` 的 chunk（与原文 chunk 并存）。  
- **检索**：`rag.py` 中 `retrieve()` 已做查询扩展（命理术语同义组）；可继续加：向量检索 → rerank → 截断。

---

## 七、白话文向量 RAG（已实现）

- **向量库**：`knowledge/vector_store/` 下 `embeddings.npy`（向量）+ `meta.json`（id/source/content）。  
- **构建**：在 `backend` 目录执行 `python scripts/build_vector_store.py`。首次运行会下载多语言 embedding 模型（约 400MB），并对全部白话 chunk 做向量化，约需数分钟。  
- **检索逻辑**：`rag.retrieve()` 优先用向量相似度检索白话知识库；若未构建向量库或检索为空，则回退到关键词/术语扩展检索。  
- **依赖**：`sentence-transformers`、`numpy`（见 `backend/requirements.txt`）。
